《编写可读代码艺术》第十二章，标题是把想法变成代码。其实个人更喜欢另一种描述 —— 小黄鸭编码法。

听过小黄鸭调试法就大概懂了。小黄鸭调试法是：

```
当程序员面对 bug 却不可自查，陷入困顿时。在电脑旁放一只小黄鸭，一边抚摸它一边讲述自己的代码思路与流程，希望借此找到代码的漏洞。
```

所以小黄鸭编码法就是 **面对一段需要实现的功能，先自己想一遍，说一遍功能的大致流程与细节，然后再对应地编码**。

书中也提到，其实写程序，就是人利用计算机语言来向计算机解释一件事情，就好像我们跟朋友讲述一个故事。所以自己说一遍自己要写的东西，是有帮助的。

小黄鸭编码法对可读性的提高有所帮助，尤其是一些较为复杂的逻辑判断，这个在业务中也是常见的校验流程。

如果发现自己代码整体有点复杂冗长，就更应该先小黄鸭一下，理清楚思路，而不是盲目上手。提高可读性的方法是很多，但是从一开始就规范起来，可以少走弯路。

```lua
function QuestionSystem:giveNextQuestion(questionId, isAnswerCorrect)
    local question = {
        isEnd           = false,
        maxAnswerTime   = 11,
        nextQuestionId  = 0,
    }
    
    local questionConfig = Config.getConfById(questionId)
    if questionConfig.nextType == NEXT_TYPE.ANY
    or (isAnswerCorrect
    and questionConfig.nextType == NEXT_TYPE.RIGHT) then
        if not self:isEnd() then
            self._count = self._count + 1
            question.isEnd = false
            -- 随机一个
            if questionConfig.nextQuestionId == -1 then
                question.nextQuestionId = math.random(1, questionConfig.maxNum)
            else
                question.nextQuestionId = questionConfig.nextQuestionId
            end
            question.maxAnswerTime = Config.getConfById(question.nextQuestionId).maxAnswerTime
            return question
        else
            question.isEnd = true
            return question
        end
    end
end
```

大概写成这样，可能就要反思一下，开始小黄鸭了(很偷懒，这是之前写的一篇炸鸡的例子)：

```
返回一个 next question。
question 有跳转类型，any 和 right 才有 next question 的数据，如果是 right, 需要答对才能继续。
同时回答题目没有结束。(not isEnd)
没结束需要看 nextQuestionId, -1 的是随机，不然就是固定。
```

当然，这个是初稿，就完美重现上述代码。那么我们再想想该怎么写：

```
回答结束了，就直接结束了。
没结束需要看跳转类型，不符合就没数据返回。
下一题需要确定随机还是固定生成。
```

然后直接写下来。

```lua
function QuestionSystem:giveNextQuestion(questionId, isAnswerCorrect)
    local question = {
        isEnd           = false,
        maxAnswerTime   = 11,
        nextQuestionId  = 0,
    }
    if self:isEnd() then
        question.isEnd = true
        return question
 		end
  
  	
    local questionConfig = Config.getConfById(questionId)
  	local nextAnyway     = questionConfig.nextType == NEXT_TYPE.ANY
  	local nextNeedRight  = (isAnswerCorrect and questionConfig.nextType == NEXT_TYPE.RIGHT)
    if not nextAnyway
    and not nextNeedRight then
    	return question
    end
  
  	self._count = self._count + 1
    question.isEnd = false
    if questionConfig.nextQuestionId == -1 then
      question.nextQuestionId = math.random(1, questionConfig.maxNum)
    else
      question.nextQuestionId = questionConfig.nextQuestionId
    end
    question.maxAnswerTime = Config.getConfById(question.nextQuestionId).maxAnswerTime
    return question
end
```

所以我觉得小黄鸭对条件判断多的情况比较有帮助。



# ML(待补)

## pytorch tensor

这个大多我都放到 [github](https://github.com/MultiParam/reach_goal/blob/master/dapao/2020-08-03-08-05/tensor.ipynb) 上了，`jupyter-notebook` 上也有相应的笔记。但是手写也可以加深一些印象。

在数据预处理这个阶段，我目前也只接触用 `pandas` 来处理数据，然后归一化处理。而在 `DL` 中，常使用张量来处理一些原始数据，从而可以放入神经网络训练。

我就列一些 `pytorch` 的张量的基本用法。`pytorch` 中的张量其实都是 `Tensor` 类的实例，我们可以通过如下几种方式来生成一个张量实例。

### 用已有数据创建张量

```python
import torch as t
import numpy as np

data = np.array([1, 2, 3])

t.Tensor(data) # tensor([1., 2., 3.])
t.tensor(data) # tensor([1, 2, 3], dtype=torch.int32)
t.as_tensor(data) # tensor([1, 2, 3], dtype=torch.int32)
t.from_numpy(data) # tensor([1, 2, 3], dtype=torch.int32)
```

可以见得，第一种方法是利用构造方法生成实例，但是张量中的数据类型并不是跟着传入的 `nmupy.darray` 走的。说明这种方法创造的张量，数据类型是系统默认的数据类型。

而剩下几种方式，都是工厂方法函数来创建的(工厂方法，一种设计模式)，它们会根据传入的数据类型来决定自己的数据类型。

### 四种方式的深层次差别

除了上文写的差别外，其实还有内存表现上的差别。

我们把 `data` 修改一下：

```python
print('old', data)
data[0] = 0
data[1] = 0
data[2] = 0
print('new', data)
print(t1)
print(t2)
print(t3)
print(t4)
```

结果是这样的：

```python
old [1 2 3]
new [0 0 0]
tensor([1., 2., 3.])
tensor([1, 2, 3], dtype=torch.int32)
tensor([0, 0, 0], dtype=torch.int32)
tensor([0, 0, 0], dtype=torch.int32)
```

修改了源数据，`as_tensor,from_numpy` 方式创建的张量的数据也被修改。说明这两种方式，是和源数据共享内存的，而 `Tensor, tensor` 方式都是拷贝内存。 

需要注意的是，`as_tensor` 方法对 `numpy.ndarray` 共享内存，不对 `python` 内建数据类型共享。

```python
l = [1, 2, 3]
t5 = t.as_tensor(l)
print(t5)
l[0] = 0
print(l)
print(t5)

# 结果
tensor([1, 2, 3])
[0, 2, 3]
tensor([1, 2, 3])
```

推荐的创建张量方式一般来说是 `tensor`。如果考虑性能，可以使用 `as_tensor`。

## 未写完的东西

其实还有学到了 `CNN` 的张量形状，以及其由来。同时还学到了 `flatten, reshape` 等操作。但是鉴于时间不多，就留到下次的总结。

果然当时的输入就要快快的做好笔记，不然万事蹉跎。