# ML

其实前两份总结，的确是讲了一下机器学习大致的三个步骤，而且也说了一下梯度下降大概是什么样的。

但是忽略了挺多东西的，比如什么是 `train/test data`，什么是 `过拟合欠拟合` 等等。而概念的搞懂对于后头的实践还是有帮助的，所以我想想到什么细节就补充一下。

首先我们针对一个问题。

1. 选择了一个函数集合(未定参数的模型)。$f(w) = wx$
2. 确定 `loss function`。$L(w) = \sum(\hat{y} - wx)^2$
3. 找到最好的函数(梯度下降)

而梯度下降的步骤之前也大致写了，也不赘述，如下是参数 $w$ 的梯度公式：
$$
L(w) = 2\sum(y - wx)(-x)
$$
那么这个 $x, y$ 从哪而来。就是从`train data` ，也就是训练集而来。而且是多个 `samlpe`，越多的 `samlpe` 越能接近普遍情况：
$$
X_{train} =\begin{bmatrix}
122\\
22\\
33\\
...
\end{bmatrix}
Y_{train} =\begin{bmatrix}
1.24\\
2.355\\
3.3334\\
...
\end{bmatrix}
$$
训练数据可以是图像像素，可以是一个表格提取的数据，总之训练数据会被代入梯度公式，一步一步训练，得到一个形状确定的模型。
$$
y = 0.1x\\
(w = 0.1)\\
$$
当然，得到模型后并不是可以拍手称快了。我们以上做的只是针对 `train data` ，模型在其上的表现可能是不错的，但是我们需要模型就是为了做预测，或者说发现一个潜在的规律。

也就是我们关心模型的 **泛化** 能力。

那么我们现在的模型，是否准确需要如何衡量？

那就给另外一组数据，来检测一下模型预测的如何，而这组数据就叫做 `test data`，测试集。例如我们有一组`test data`
$$
X_{test} =\begin{bmatrix}
12\\
22\\
33\\
...
\end{bmatrix}
Y_{test} =\begin{bmatrix}
1.2\\
2.3\\
3.34\\
...
\end{bmatrix}
$$
将这些数据代入训练好的模型，利用定义好的 `loss function` 来衡量模型面对未知数据的准确性。

在理论上，我们会根据模型在 `train\test data`  上的表现组合得到如下结果：

|           | train bad                | train good            |
| --------- | ------------------------ | --------------------- |
| test good | ~~train bad, test good~~ | train good, test good |
| test bad  | train bad, test bad      | train good, test bad  |

我们的目标：`train good, test good`，也就是在训练集上效果好，也要在测试集上效果好。但是天总不是随人愿的，会出现 `train good, test bad` 和 `train bad, test bad`两种情况。(至于表格中被划线的情况，因为个人认为在 `train data` 上表现不好，更不用谈在测试集上表现更好了。当然这一点我也不是特别确定，欢迎指正。)

 先从 `train good, test bad`说起。一般说来，通过我粗浅的训练操作，`loss` 会随着不断地迭代训练而减少，可以叫做不断地让模型效果变好。而这个变好是建立在 `train data`上的，也可以说，是在 `train data` 上有好的表现，但是不一定在 `test data`上有好的表现。

所以如果出现这种情况，其实就叫做 `过拟合`。模型过分在意 `train data`而导致泛化能力下降。

那么 `train bad, test bad` 说明，目前训练的模型，在 `train data` 上都表现不好，学习的还不够。这叫做 `欠拟合`。

所以这样就造成了一种需要在两者之间权衡的情况，这真的很像人生哲学智慧，不得不说古人的抽象泛化能力是很有造诣的。

~~既然要权衡欠拟合与过拟合，其实就是权衡模型预测的损失怎样才能在两处都较小。~~

其实最好的承上启下是：为了向 `train good, test good` 的方向努力，要怎么做。

**降低** 损失，**提高**准确度。`train bad -> train good -> test bad -> test good`



~~那么怎么降低损失，就需要分析损失，这样才能指导模型优化的方向。在李宏毅老师的教学中，他认为 `error comes from bias and variance`。~~

~~在说这个之前，的确需要明确一件事情。假设有一个最佳的函数，它是我们要找的规律 $\hat{f}$。我们训练的结果是一个模型， 一个函数 $f^*$，它可以不断通过训练逼近 $\hat{f}$。 但是这是一次训练的结果，即使不变化定义的函数集合，不同的训练数据会都产生不一样的模型，可能有更好效果的 $f^*$ 在远方等候也不一定啊。~~

~~所以训练是多次的，会产生$f^*_1, f^*_2, ...$，我们就需要做选择，择优录取。~~



其实上三段是要展开李宏毅老师的第二章内容，`where error comes from`。但其实还是主要讲过拟合，欠拟合，通过介绍 `bias` 和 `variance` 两个概念开始，赘述就太长了，还是简短一些。

这一章思路主要是介绍了：

1. `error` 由 `bias` 和 `variance` 而来。`bias` 表现了训练模型与最佳模型之间的期望差距。`variance` 更好理解，其实就表现了数据的波动程度。

2. 模型的复杂程度与 `bias, variance` 之间的联系。越复杂的模型 `bias` 越小，因为它包含的数据更多，所以更接近最佳模型的期望，但 `variance` 就容易偏大，因为复杂的模型对于数据细微的变动是很敏感的。

3. 通过模型与 `bias, variance`的联系引出与欠拟合、过拟合的关系，以及接下来的优化方向。 如果 `bias`大，`variance`小，说明模型还是比较简单的，容纳情况不足，还是欠拟合。

   这种情况就需要：

   1. 增加 `feature`。
   2. 增加 `train data`。

4. 如果出现 `bias` 小，`variance` 大，则可能是出现过拟合。

   这种情况就可以：

   1. 增加 `train data`。真万能。
   2. 为了让 `variance` 下降，需要让模型对数据的变化更平滑一些，在梯度公式加入正则项。

5. 训练多次产生的多个模型的选择。交叉验证、n 折交叉验证等。

写到这里，也发现自己有理解的地方，也有不理解的地方。但是对于概念理解和整个训练流程的了解还是有裨益的，当然这与实践产生的理解相比还是相去甚远。

而以上提到的就是一些需要理解的概念，还有训练的优化方向。而后头会写的梯度下降的优化，则是训练优化方向中的一部分。

两者的关系就好像是总路线，与总路线指导下的思想。姑且这么理解吧。

# 可读代码编写

大致地过了一下《编写可读代码的艺术》第十一章。为什么突然接着看这本书，也是因为想把这个系列完结了，同时，晚上的加班并不想为公司工作，而是为自己学习，看看书也是极好的。

这一章其实讲的就是这个内容：

![](http://img.multiparam.com/dapao/code/20200729121503.png)

对的，就是这么简单，然后作者通过几个例子来提供对比，从而证明这样的方法对代码的可读性有提升作用。

其实在之前我写的文章中，就有类似的处理。当函数中有几段逻辑同时存在时，可以选择把这几段逻辑分段开，以表现函数中不同的逻辑分工。

而一次只做一件事是更加强调了这一点，我们可以考虑一种代码，里头有 `A,B,C`三个主要功能代码块，而是哪个代码块中又有三个代码语句，例如 `A` 中有 `A-1,A-2,A-3`  三个语句。

如果代码是这样：

```lua
function xxx()
	A-1
  B-1
  A-2
  
  C-2
  ...
  C-1
  
  A-3
end
```

我不用具体的代码，就这样的伪代码，就已经让人感到眼花缭乱了。要是嵌套再深一些，是更加难以理解，阅读的可读性大打折扣。

但是如果分开的代码段中，做的事情是为一个目标服务的，那么就是在做一件事。如下代码所示，`A-1\2\3`是为同一个目标服务，在做一件事，所以放在一起更容易理解。同时这几件事又通过分段区分开，提升了可读性。

```lua
function xxx(info)
  A-1
  A-2
  A-3
  
  B-1
  B-2
  B-3
  ...
  
end
```



书中也提供了一幅图来描述这样的对比，更加直观：

![](http://img.multiparam.com/dapao/code/20200729122909.png)

当然把上述例子在扩大，就是可以将一个过于庞大的代码段，封装成类\函数，来表达一件事情。