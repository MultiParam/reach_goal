不知不觉你已经离开我？

其实需要承认一点，我现在算是对自己库存的知识做一个重温总结，这不免有一些偷懒的嫌疑。这个我会逐渐赶上，毕竟面试已经过不去了，指望朋友的内推**不如自己的精进**。

总结的过程中逐渐发现数学的知识欠缺，但是这个在以后慢慢补足，广义上的理解需要更快进行，不然很难走到写代码，做应用的步骤。

其实，想了一想，```loss function``` 是 ```function``` 的 ```function```，这句话一点毛病都没有。一组参数 ($w, b$) 的变化决定了不同的 ```function```，也决定了不同的 ```loss function```。

# 梯度下降的粗浅解释

[我上一篇总结](2020-07-20-22.md) 主要讲了机器学习的三个步骤，在第三步 —— 找到最好的函数时，提到了一个方法叫做 **梯度下降**。

其实找到最好的函数就转换为找到一组参数使得 ```loss function``` 得到的损失是最小的。

那么我们可以先假设，选择了一个的模型/函数集合 $f(w)$ 的 ```loss function``` 为 $L(w)$，它只有一个参数$w$。为了达到我们的目的，就要找到一个 $w^*$ ，它可以得到极小值/最小值。
$$
w^* = \arg{\underset{w}{min}}L(w)
$$
所以第一反应是对 $L(w)$ 关于 $w$ **求导**，找到其导数为 0 的点，同时还要保证是最低点。接下来可能有点抽象，但是我又不太会 gif 图制作，就一步一步说吧。

1. 给 $w$ 一个随机值 $w_0$。
2. 计算 $\left.\frac{\mathrm{d}L}{\mathrm{d}w}\right|_{w=w_0}$
3.  $\left.\frac{\mathrm{d}L}{\mathrm{d}w}\right|_{w=w_0}$ 如果为负数，说明在这个点，$L(w)$ 的变化趋势是减少， 所以 $w$ 需要增加，让 $L(w)$ 继续减少。$\left.\frac{\mathrm{d}L}{\mathrm{d}w}\right|_{w=w_0}$ 如果为正数，说明在这个点，$L(w)$ 的变化趋势是增加，所以 $w$ 需要减小，让 $L(w)$ 继续减少。

![截图自李宏毅老师的教学 PPT(1)](http://img.multiparam.com/dapao/code/20200725093805.png)

4. 那么需要让 $w_0$ 的值如何变化？ 其实就是 $w_1 = w_0 - \eta\left.\frac{\mathrm{d}L}{\mathrm{d}w}\right|_{w=w_0}$。$\eta$ 叫做 **学习率**，可以理解为变化的**步长**。

   ![截图自李宏毅老师的教学 PPT(2)](http://img.multiparam.com/dapao/code20200727224205.png)

5. 利用得到的 $w_1$ ，再执行 2 - 5 步骤，不断循环，最终得到一个最终的 $w_T$。

![截图自李宏毅老师的教学 PPT(3)](http://img.multiparam.com/dapao/code/20200725100144.png)

而得到了这个 $w_T$，模型也就确定了。

那么如果是多参数的模型，例如线性回归模型 $f(w, b) = b + \sum{wixi}(i = 1, ... m)$，其```loss function``` 选择 $L(w, b) = \frac{\sum_{i=0}^n(\hat{y^n} - f(w,b))^2}{2n} (n = 1,2,3 ...)\\$ 就是对每个参数求偏导数 $w_{grad} = [\left.\frac{\partial L(w, b)}{\partial w_1}\right., \left.\frac{\partial L(w, b)}{\partial w_2}\right.,..., \left.\frac{\partial L(w, b)}{\partial w_m}\right., \left.\frac{\partial L(w, b)}{\partial b}\right]$，通过如上步骤，最终得到一组参数 $w = [w_1, w_2, ..., w_m]$，从而得到一个模型。

而这些各参数的偏导：
$$
w_{grad} = [\left.\frac{\partial L(w, b)}{\partial w_1}\right., \left.\frac{\partial L(w, b)}{\partial w_2}\right.,..., \left.\frac{\partial L(w, b)}{\partial w_m}\right., \left.\frac{\partial L(w, b)}{\partial b}\right]
$$
就被称作梯度：
$$
\bigtriangledown L = 
\begin{bmatrix}
\left.\frac{\partial L(w, b)}{\partial w_1}\right.\\
\left.\frac{\partial L(w, b)}{\partial w_2}\right.\\
\left.\frac{\partial L(w, b)}{\partial w_m}\right.,\\
...\\
\left.\frac{\partial L(w, b)}{\partial b}\right. \\
\end{bmatrix}
$$
所以个人觉得，稍微理解一些线性代数是有用的，更好地理解一些公式，同时涉及相关编码时，例如使用 ```numpy``` 操作多 ```feature``` 变量时，善用矩阵操作而不是循环操作是很重要的。

